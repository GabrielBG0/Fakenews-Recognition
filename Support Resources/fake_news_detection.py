# -*- coding: utf-8 -*-
"""Fake News Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Dov0wuFZ9aLYKswBSGeNHTX1xq-_u4d
"""

import copy
from transformers import BertTokenizerFast
from sklearn.model_selection import train_test_split
import pickle
import numpy as np


documents = pickle.load(open("FakeNewsNet/dict_documents.pkl", "rb"))
indexes = np.load('FakeNewsNet/indexes.npy')
lbls = np.load('FakeNewsNet/labels.npy')

texts = []
labels = []
for i in range(len(indexes)):
    index = indexes[i]
    texts.append(documents[index])
    if lbls[i] == np.str_("-1"):
        labels.append(0)
    else:
        labels.append(1)

print("lable 1: " + str(labels[0]))
print("text 1: " + texts[0])

count = 0
for text in texts:
    if len(text.split(' ')) > 512:
        count += 1
print(count)

train_texts, test_texts, train_labels, test_labels = train_test_split(
    texts, labels, test_size=.2)
train_texts, val_texts, train_labels, val_lables = train_test_split(
    train_texts, train_labels, test_size=.2)

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(train_texts, padding=False)
val_encodings = tokenizer(val_texts, padding=False)
test_encodings = tokenizer(test_texts, padding=False)

len(train_encodings["input_ids"][15])

# CLS token id = 101, SEP token id = 102


def truncate(encodings, method="h", max_len=512):
    aux = copy.deepcopy(encodings)

    if method == "t":
        print('methond selected was: tail')
        for encoding in aux["input_ids"]:
            if len(encoding) > max_len:
                aux_input_ids = [101, *encoding[-max_len-1:]]
                encoding = aux_input_ids.copy()
        for encoding in aux["token_type_ids"]:
            if len(encoding) > max_len:
                aux_token_type_ids = [encoding[0], *encoding[-max_len-1:]]
                encoding = aux_token_type_ids.copy()
        for encoding in aux["attention_mask"]:
            if len(encoding) > max_len:
                aux_attention_mask = [encoding[0], *encoding[-max_len-1:]]
                encoding = aux_attention_mask.copy()

    elif method == "h":
        print('methond selected was: head')
        for encoding in aux["input_ids"]:
            if len(encoding) > max_len:
                aux_input_ids = [*encoding[max_len-1:]]
                aux_input_ids.append(102)
                encoding = aux_input_ids.copy()
        for encoding in aux["token_type_ids"]:
            if len(encoding) > max_len:
                aux_token_type_ids = [*encoding[max_len-1:]]
                aux_token_type_ids.append(encoding[-1])
                encoding = aux_token_type_ids.copy()
        for encoding in aux["attention_mask"]:
            if len(encoding) > max_len:
                aux_attention_mask = [*encoding[max_len-1:]]
                aux_attention_mask.append(encoding[-1])
                encoding = copy.copy(aux_attention_mask)

    elif method == "ht":
        print('methond selected was: head + tail')
        head_len = int(max_len * 0.25)
        tail_len = int(max_len * 0.75)
        for encoding in aux["input_ids"]:
            if len(encoding) > max_len:
                aux_input_ids = [*encoding[head_len:]]
                aux_input_ids = [*aux_input_ids, *encoding[-tail_len:]]
                encoding = aux_input_ids.copy()
        for encoding in aux["token_type_ids"]:
            if len(encoding) > max_len:
                aux_token_type_ids = [*encoding[head_len:]]
                aux_token_type_ids = [
                    *aux_token_type_ids, *encoding[-tail_len:]]
                encoding = aux_token_type_ids.copy()
        for encoding in aux["attention_mask"]:
            if len(encoding) > max_len:
                aux_attention_mask = [*encoding[head_len:]]
                aux_attention_mask = [
                    *aux_attention_mask, *encoding[-tail_len:]]
                encoding = copy.copy(aux_attention_mask)

    return aux


train_encodings = truncate(train_encodings, method="t", max_len=512)

len(train_encodings["input_ids"][15])
