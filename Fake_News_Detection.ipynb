{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fake News Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielBG0/Fakenews-Recognition/blob/main/Fake_News_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW29XiNWddAm",
        "outputId": "00dc8fef-3752-4d97-89b9-a6902ae77a46"
      },
      "source": [
        "# Deleting default sample data folder\n",
        "!rm -rf /content/sample_data\n",
        "!rm -rf /content/FakeNewsNet\n",
        "!wget https://github.com/GabrielBG0/Fakenews-Recognition/raw/main/FakeNewsNet.zip\n",
        "!unzip FakeNewsNet.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-14 02:29:55--  https://github.com/GabrielBG0/Fakenews-Recognition/raw/main/FakeNewsNet.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GabrielBG0/Fakenews-Recognition/main/FakeNewsNet.zip [following]\n",
            "--2021-05-14 02:29:55--  https://raw.githubusercontent.com/GabrielBG0/Fakenews-Recognition/main/FakeNewsNet.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28359746 (27M) [application/zip]\n",
            "Saving to: ‘FakeNewsNet.zip.1’\n",
            "\n",
            "FakeNewsNet.zip.1   100%[===================>]  27.05M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-14 02:29:55 (223 MB/s) - ‘FakeNewsNet.zip.1’ saved [28359746/28359746]\n",
            "\n",
            "Archive:  FakeNewsNet.zip\n",
            "  inflating: FakeNewsNet/dict_documents.pkl  \n",
            "  inflating: FakeNewsNet/indexes.npy  \n",
            "  inflating: FakeNewsNet/labels.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jR_9XzOdqey",
        "outputId": "b6e14e95-df1d-4ef9-ca9a-8280e3a3dca6"
      },
      "source": [
        "# Install aditional librarys\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.6.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.4.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA-POw6sds6b",
        "outputId": "4d868562-085e-40fc-fdec-32e03542cf13"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "documents = pickle.load(open(\"FakeNewsNet/dict_documents.pkl\", \"rb\"))\n",
        "indexes = np.load('FakeNewsNet/indexes.npy')\n",
        "lbls = np.load('FakeNewsNet/labels.npy')\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "for i in range(len(indexes)):\n",
        "    index = indexes[i]\n",
        "    texts.append(documents[index])\n",
        "    if lbls[i] == np.str_(\"-1\"):\n",
        "      labels.append(0)\n",
        "    else :\n",
        "      labels.append(1)\n",
        "\n",
        "print(\"lable 1: \" + str(labels[0]))\n",
        "print(\"text 1: \" + texts[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lable 1: 0\n",
            "text 1: When I revealed in this column last year that Dame Helen Mirren was the new face of L’Oreal, I had hundreds of letters from readers, ecstatic that beauty giants are celebrating older women.  As these exclusive pictures show, from next week (February 5) Helen will be appearing in her first television advertising campaign for the company.  The 69-year-old actress will be seen championing the role of the older woman — and the candid, unretouched shots from the advert show that wrinkles, ageing hands and natural make-up can be beautiful.  Scroll down for video  Dame Helen Mirren photographed for the L'Oreal Paris age perfect campaign. In this shot Helen wears the Age Perfect day cream, the candid, unretouched shots are a refreshing change and champion the beauty of the older woman  I suspect this approach will sell more products than the glossier, airbrushed campaigns that many women feel don’t relate to them and their real lives.  During the filming of the advert, Dame Helen took time out to speak exclusively to us about what she really thinks about beauty — and reveals she doesn’t even like the word!  She told us: ‘I used to worry a lot more about my looks than I do now.  ‘I think the great advantage of getting older is that you let go of certain things. Having said that, I think all women worry to some degree — and I don’t think men are exempt.  ‘I don’t really like the word “beauty”. There are physically beautiful people in the world (David Beckham, for instance, is unbelievably beautiful), then there are other people that are not beautiful, but are very attractive because of their personality, energy, brilliance, genius: all kinds of things.  Helen (pictured for the L'Oreal campaign) says she used to worry about her looks but as she's aged she's learnt to let go of certain things  ‘So I have a resistance to the word “beautiful”. I wish we could find another word that takes it away from physical beauty and brings it more into the world of true attractiveness.’  Dame Helen also told of her shock at being asked to be the face of L’Oreal. ‘I was surprised — but flattered as I am a fan of the brand and its products.  ‘I love make-up. I’m often seen prowling the aisles of various chemists all over the world looking for a new lipstick or a new mascara, so to become a spokesperson for a major cosmetics firm is very exciting.  ‘I think L’Oreal’s slogan “Because I’m Worth It” really strikes a chord with women. Self-esteem is such a hugely important thing and it’s so difficult for all of us.  Dame Helen's much admired hair colour is now maintained by L'Oreal Excellence Age Perfect  ‘Everybody has moments of massive insecurity and I think anything that makes you feel more confident and more secure in yourself is a great thing. It’s tough for a lot of women with busy schedules or limited resources: they are incredibly busy and have difficult lives. To stop for a few seconds to say: “You know what, I can sit down, have a breather, a cup of tea or a bath and think “Yes, I’m worth it, my life is worth it” is so important.  ‘People say to me “Oh, you’re so self-confident” but I am not naturally self-confident, I just have had to be in my work and my life. If it’s a problem, it’s my problem — I’ve got to deal with it.’  It’s this refreshing honesty that makes Helen the perfect figurehead for older women. So was there an age at which she started to feel differently about the way she looked?  ‘My whole life has been slightly different from most people’s in the sense that I’ve always had to look at images of myself from when I was 21 years old,’ she said.  ‘I’ve either been in front of a camera or in the theatre, being photographed, having my picture in the paper, so I’ve grown up with what I look like and ageing has never sort of come as a shock.  ‘I’m not obsessive when it comes to looking after my skin. I have always used moisturiser at night and in the morning — it makes me feel better and to me that’s what I want from it.  ‘It doesn’t have to make me look ten years younger: it’s all about how it makes me feel better that day.  ‘My biggest beauty advice is just to make sure you clean your skin really well and don’t smoke. I also have to have my eight hours of sleep a night.’  While her new role as an ambassador for the beauty brand hasn’t changed Helen’s skincare regime, what has changed is her hair.  Dame Helen is pictured here at the Golden Globe awards two weeks ago, accompanying her stylist dress with a slash of bold red lipstick  For the initial print campaign, she was given a stylish crop, and her much-admired bright blonde is now maintained using L’Oreal’s new Excellence Age Perfect hair colour.  ‘I have very fine hair so I’ve always been quite careful not to overtreat it. I don’t normally colour or dye it, I like it to grow naturally, but this dye is fantastic. It really does what it says on the box, allows my hair to grow out naturally and is also very easy to use. It’s my new favourite product.  ‘Yes, I know it sounds like I’m saying it because of the advertising, but it’s true! The brilliant thing is that it has layers of colour, it’s not one colour that you then need to add highlights on top of.’  So what hair advice would she give to somebody who, like her, is in their 60s?  ‘Be bold! Be bold with your cut, really look what young people are doing and copy them — don’t copy what old people are doing.  Seen here last year, Helen is clearly a fan of a statement lip. She says her advice to women out there is to look at what's happening now style-wise and 'go with the flow'  ‘I loved it when I dyed my hair pink. A lot of women get stuck at what they are good at and what they did between the ages of 18 and 28 and they never have the courage to change that.  ‘You see women from the Sixties still with their beehives and now they’re in their 70s. You see women from the Eighties who still have those Eighties hair-dos. My advice is to look at what’s happening now and go with the flow — don’t do what you did when you were 24.  ‘Some people have a classic haircut and it works for them and that’s great, but it’s much better to accept how you look now and then be modern.’\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRRLWulldunw",
        "outputId": "2969393d-99e0-414f-ed0f-073603644a09"
      },
      "source": [
        "count = 0\n",
        "for text in texts:\n",
        "  if len(text.split(' ')) > 512:\n",
        "    count += 1\n",
        "count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo-mzL0zRjO7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=.2)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqiOpdSpRt2M"
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE3eibcIRmbX",
        "outputId": "48ab0158-7329-46bb-b122-db94c7af561f"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=False)\n",
        "val_encodings = tokenizer(val_texts, padding=False)\n",
        "test_encodings = tokenizer(test_texts, padding=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxM9rf0yft6b"
      },
      "source": [
        "import copy\n",
        "# CLS token id = 101, SEP token id = 102\n",
        "def truncate(encodings, method=\"h\", max_len=512):\n",
        "  aux = copy.deepcopy(encodings)\n",
        "\n",
        "  if method is \"t\":\n",
        "    print('methond selected was: tail')\n",
        "    #for encoding in aux[\"input_ids\"]:\n",
        "    for i in range(len(aux[\"input_ids\"])):\n",
        "      if len(aux[\"input_ids\"][i]) > max_len:\n",
        "        aux_input_ids = [101, *aux[\"input_ids\"][i][-max_len+1:]]\n",
        "        aux[\"input_ids\"][i] = aux_input_ids.copy()\n",
        "      else:\n",
        "        aux[\"input_ids\"][i] = [*aux[\"input_ids\"][i], *[0]*(max_len - len(aux[\"input_ids\"][i]))]\n",
        "    for i in range(len(aux[\"token_type_ids\"])):\n",
        "      if len(aux[\"token_type_ids\"][i]) > max_len:\n",
        "        aux_token_type_ids = [aux[\"token_type_ids\"][i][0], *aux[\"token_type_ids\"][i][-max_len+1:]]\n",
        "        aux[\"token_type_ids\"][i] = aux_token_type_ids.copy()\n",
        "      else:\n",
        "        aux[\"token_type_ids\"][i] = [*aux[\"token_type_ids\"][i], *[0]*(max_len - len(aux[\"token_type_ids\"][i]))]\n",
        "    for i in range(len(aux[\"attention_mask\"])):\n",
        "      if len(aux[\"attention_mask\"][i]) > max_len:\n",
        "        aux_attention_mask = [aux[\"attention_mask\"][i][0], *aux[\"attention_mask\"][i][-max_len+1:]]\n",
        "        aux[\"attention_mask\"][i] = aux_attention_mask.copy()\n",
        "      else:\n",
        "        aux[\"attention_mask\"][i] = [*aux[\"attention_mask\"][i], *[0]*(max_len - len(aux[\"attention_mask\"][i]))]\n",
        "\n",
        "  elif method is \"h\":\n",
        "    print('methond selected was: head')\n",
        "    for i in range(len(aux[\"input_ids\"])):\n",
        "      if len(aux[\"input_ids\"][i]) > max_len:\n",
        "        aux_input_ids = [*aux[\"input_ids\"][i][:max_len-1], 102]\n",
        "        aux[\"input_ids\"][i] = aux_input_ids.copy()\n",
        "      else:\n",
        "        aux[\"input_ids\"][i] = [*aux[\"input_ids\"][i], *[0]*(max_len - len(aux[\"input_ids\"][i]))]\n",
        "    for i in range(len(aux[\"token_type_ids\"])):\n",
        "      if len(aux[\"token_type_ids\"][i]) > max_len:\n",
        "        aux_token_type_ids = [*aux[\"token_type_ids\"][i][:max_len-1], aux[\"token_type_ids\"][i][-1]]\n",
        "        aux[\"token_type_ids\"][i] = aux_token_type_ids.copy()\n",
        "      else:\n",
        "        aux[\"token_type_ids\"][i] = [*aux[\"token_type_ids\"][i], *[0]*(max_len - len(aux[\"token_type_ids\"][i]))]\n",
        "    for i in range(len(aux[\"attention_mask\"])):\n",
        "      if len(aux[\"attention_mask\"][i]) > max_len:\n",
        "        aux_attention_mask = [*aux[\"attention_mask\"][i][:max_len-1], aux[\"attention_mask\"][i][-1]]\n",
        "        aux[\"attention_mask\"][i] = aux_attention_mask.copy()\n",
        "      else:\n",
        "        aux[\"attention_mask\"][i] = [*aux[\"attention_mask\"][i], *[0]*(max_len - len(aux[\"attention_mask\"][i]))]\n",
        "\n",
        "  elif method is \"ht\":\n",
        "    print('methond selected was: head + tail')\n",
        "    head_len = int(max_len * 0.25)\n",
        "    tail_len = int(max_len * 0.75)\n",
        "    for i in range(len(aux[\"input_ids\"])):\n",
        "      if len(aux[\"input_ids\"][i]) > max_len:\n",
        "        aux_input_ids = [*aux[\"input_ids\"][i][:head_len], *aux[\"input_ids\"][i][-tail_len:]]\n",
        "        aux[\"input_ids\"][i] = aux_input_ids.copy()\n",
        "      else:\n",
        "        aux[\"input_ids\"][i] = [*aux[\"input_ids\"][i], *[0]*(max_len - len(aux[\"input_ids\"][i]))]\n",
        "    for i in range(len(aux[\"token_type_ids\"])):\n",
        "      if len(aux[\"token_type_ids\"][i]) > max_len:\n",
        "        aux_token_type_ids = [*aux[\"token_type_ids\"][i][:head_len], *aux[\"token_type_ids\"][i][-tail_len:]]\n",
        "        aux[\"token_type_ids\"][i] = aux_token_type_ids.copy()\n",
        "      else:\n",
        "        aux[\"token_type_ids\"][i] = [*aux[\"token_type_ids\"][i], *[0]*(max_len - len(aux[\"token_type_ids\"][i]))]\n",
        "    for i in range(len(aux[\"attention_mask\"])):\n",
        "      if len(aux[\"attention_mask\"][i]) > max_len:\n",
        "        aux_attention_mask = [*aux[\"attention_mask\"][i][:head_len], *aux[\"attention_mask\"][i][-tail_len:]]\n",
        "        aux[\"attention_mask\"][i] = aux_attention_mask.copy()\n",
        "      else:\n",
        "        aux[\"attention_mask\"][i] = [*aux[\"attention_mask\"][i], *[0]*(max_len - len(aux[\"attention_mask\"][i]))]\n",
        "\n",
        "  return aux"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jMTq8eKcZcV",
        "outputId": "5b7e4f04-044a-4283-9bfe-2707f0e1a5bd"
      },
      "source": [
        "train_encodings = truncate(train_encodings, method=\"t\", max_len=128)\n",
        "val_encodings = truncate(val_encodings, method=\"t\", max_len=512)\n",
        "test_encodings = truncate(test_encodings, method=\"t\", max_len=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "methond selected was: head + tail\n",
            "methond selected was: head + tail\n",
            "methond selected was: head + tail\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPIhKM-2WROv"
      },
      "source": [
        "import torch\n",
        "\n",
        "class FNNDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = FNNDataset(train_encodings, train_labels)\n",
        "val_dataset = FNNDataset(val_encodings, val_labels)\n",
        "test_dataset = FNNDataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkM7DDI4XNQ0",
        "outputId": "ccde96b8-85b2-4793-fd6c-79dd5a0388d0"
      },
      "source": [
        "# Checks if there is a gpu available\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
        "else:\n",
        "    print('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXbv3Z82X0Ac",
        "outputId": "527d4cc7-1d0a-4f8a-8a0f-455498974b3a"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsEvElsh3Mbn"
      },
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(model, eval_dataloader, metric):\n",
        "  model.eval()\n",
        "  for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "  return metric.compute()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uDFDT_iXZoS",
        "outputId": "3666d07d-331d-4ad1-fc45-2c35fe040382"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(16):\n",
        "  model.train()\n",
        "  for batch in train_loader:\n",
        "    optim.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs[0]\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "  print('eval accuracy results for epoch ' + str(epoch) + ': ' + str(compute_metrics(model, val_dataloader, load_metric('accuracy'))['accuracy']))\n",
        "  print('')\n",
        "\n",
        "model.eval()\n",
        "print('trainig has finished')\n",
        "print('-------------------------------------------------------------------')\n",
        "print('final evaluation is: ')\n",
        "whantedStatistics = ['accuracy', 'f1', 'precision', 'recall']\n",
        "for statistic in whantedStatistics:\n",
        "  metric = load_metric(statistic)\n",
        "  print(statistic + ': ' + str(compute_metrics(model, test_dataloader, metric)[statistic]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eval accuracy results for epoch 0: 0.8224799286351472\n",
            "\n",
            "eval accuracy results for epoch 1: 0.848349687778769\n",
            "\n",
            "eval accuracy results for epoch 2: 0.83942908117752\n",
            "\n",
            "eval accuracy results for epoch 3: 0.8331846565566459\n",
            "\n",
            "eval accuracy results for epoch 4: 0.8385370205173952\n",
            "\n",
            "eval accuracy results for epoch 5: 0.8421052631578947\n",
            "\n",
            "eval accuracy results for epoch 6: 0.8421052631578947\n",
            "\n",
            "eval accuracy results for epoch 7: 0.8331846565566459\n",
            "\n",
            "eval accuracy results for epoch 8: 0.8287243532560215\n",
            "\n",
            "eval accuracy results for epoch 9: 0.8385370205173952\n",
            "\n",
            "eval accuracy results for epoch 10: 0.8447814451382694\n",
            "\n",
            "eval accuracy results for epoch 11: 0.8322925958965209\n",
            "\n",
            "eval accuracy results for epoch 12: 0.8349687778768956\n",
            "\n",
            "eval accuracy results for epoch 13: 0.8376449598572703\n",
            "\n",
            "eval accuracy results for epoch 14: 0.8412132024977699\n",
            "\n",
            "eval accuracy results for epoch 15: 0.8429973238180196\n",
            "\n",
            "trainig has finished\n",
            "-------------------------------------------------------------------\n",
            "final evaluation is: \n",
            "accuracy: 0.8529621698786581\n",
            "f1: 0.652027027027027\n",
            "precision: 0.7228464419475655\n",
            "recall: 0.5938461538461538\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}